{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.mixed_precision import Policy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# --- GPU CONFIGURATION FOR RTX 3050 ---\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Configure GPU memory growth for RTX 3050 (4GB VRAM)\n",
    "def configure_gpu():\n",
    "    \"\"\"Configure GPU settings for RTX 3050\"\"\"\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Set memory growth to avoid OOM errors\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "            logger.info(f\"‚úÖ GPU detected: {gpus[0].name}\")\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            logger.error(f\"‚ùå GPU configuration error: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        logger.warning(\"‚ö†Ô∏è No GPU detected, using CPU\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Configure GPU\n",
    "gpu_available = configure_gpu()\n",
    "\n",
    "# Enable mixed precision training for RTX 3050 (supports Tensor Cores)\n",
    "if gpu_available:\n",
    "    try:\n",
    "        policy = Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "        logger.info(f\"Mixed precision policy: {policy.name}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Mixed precision not available: {e}\")\n",
    "        policy = Policy('float32')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# --- 1. CONFIGURATION AND SETUP ---\n",
    "# Paths\n",
    "DATA_DIR = \"./data/prepared/images\"\n",
    "VOLUME_FILES_DIR = \"./data/prepared/volumes\"\n",
    "MODEL_SAVE_PATH = \"./models/mangosteen_volume_model_rtx3050.h5\"\n",
    "CHECKPOINT_DIR = \"./models/checkpoints\"\n",
    "LOG_DIR = \"./logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Model parameters optimized for RTX 3050 (4GB VRAM)\n",
    "IMG_SIZE = 224  # EfficientNet optimal size\n",
    "BATCH_SIZE = 12  # Reduced for 4GB VRAM with safety margin\n",
    "INITIAL_EPOCHS = 25\n",
    "FINE_TUNE_EPOCHS = 35\n",
    "LEARNING_RATE_INITIAL = 1e-3\n",
    "LEARNING_RATE_FINE_TUNE = 1e-5\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "# --- 2. DATA LOADING AND PREPARATION ---\n",
    "\n",
    "def load_data_from_folders(data_dir, volume_dir):\n",
    "    \"\"\"\n",
    "    Loads images and their corresponding volume values with error handling.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    volumes = []\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        logger.error(f\"Image data directory not found: {data_dir}\")\n",
    "        return [], []\n",
    "    if not os.path.exists(volume_dir):\n",
    "        logger.error(f\"Volume data directory not found: {volume_dir}\")\n",
    "        return [], []\n",
    "\n",
    "    valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')\n",
    "    skipped_files = 0\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.lower().endswith(valid_extensions):\n",
    "            base_filename = os.path.splitext(filename)[0]\n",
    "            volume_filename = f\"{base_filename}.txt\"\n",
    "            volume_filepath = os.path.join(volume_dir, volume_filename)\n",
    "\n",
    "            if os.path.exists(volume_filepath):\n",
    "                try:\n",
    "                    with open(volume_filepath, 'r') as f:\n",
    "                        volume_value = float(f.read().strip())\n",
    "\n",
    "                    # Validate volume value\n",
    "                    if volume_value <= 0 or volume_value > 10000:\n",
    "                        logger.warning(f\"Suspicious volume value {volume_value} for {filename}\")\n",
    "                        skipped_files += 1\n",
    "                        continue\n",
    "\n",
    "                    image_paths.append(os.path.join(data_dir, filename))\n",
    "                    volumes.append(volume_value)\n",
    "\n",
    "                except (ValueError, FileNotFoundError) as e:\n",
    "                    logger.warning(f\"Skipping {filename}: {e}\")\n",
    "                    skipped_files += 1\n",
    "            else:\n",
    "                logger.debug(f\"No volume file for {filename}\")\n",
    "                skipped_files += 1\n",
    "\n",
    "    logger.info(f\"Loaded {len(image_paths)} samples, skipped {skipped_files} files\")\n",
    "    return image_paths, volumes\n",
    "\n",
    "\n",
    "# Load data\n",
    "logger.info(\"Loading data...\")\n",
    "image_paths, volumes = load_data_from_folders(DATA_DIR, VOLUME_FILES_DIR)\n",
    "\n",
    "if not image_paths:\n",
    "    logger.error(\"No matching image and volume data found. Please check your paths.\")\n",
    "    exit()\n",
    "\n",
    "# Normalize volumes for better training stability\n",
    "volumes = np.array(volumes)\n",
    "volume_mean = np.mean(volumes)\n",
    "volume_std = np.std(volumes)\n",
    "volumes_normalized = (volumes - volume_mean) / volume_std\n",
    "\n",
    "logger.info(f\"Volume statistics - Mean: {volume_mean:.2f}, Std: {volume_std:.2f}\")\n",
    "\n",
    "# Split data\n",
    "train_paths, val_paths, train_volumes, val_volumes = train_test_split(\n",
    "    image_paths, volumes_normalized, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "\n",
    "logger.info(f\"Training samples: {len(train_paths)}, Validation samples: {len(val_paths)}\")\n",
    "\n",
    "\n",
    "# --- 3. OPTIMIZED DATA PIPELINE ---\n",
    "\n",
    "@tf.function\n",
    "def preprocess_image(image_path, volume, is_training=True):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
    "    image.set_shape([None, None, 3])           # ‚úÖ ‡πÅ‡∏ó‡∏ô reshape\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    if is_training:\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_brightness(image, 0.1)\n",
    "        image = tf.image.random_contrast(image, 0.9, 1.1)\n",
    "        image = tf.image.rot90(image, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "\n",
    "    # ‡∏Ñ‡∏•‡∏¥‡∏õ‡πÑ‡∏ß‡πâ‡πÄ‡∏â‡∏¢ ‡πÜ ‡∏Å‡πá‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏õ‡∏Å‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏á preprocess ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô [-1,1]\n",
    "    # ‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ EfficientNet preprocess ‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà critical\n",
    "    image = tf.clip_by_value(image, 0.0, 255.0)\n",
    "    image.set_shape([IMG_SIZE, IMG_SIZE, 3])   # ‚úÖ final static shape\n",
    "\n",
    "    return image, volume\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(paths, volumes, batch_size, is_training=True):\n",
    "    \"\"\"Create optimized tf.data pipeline\"\"\"\n",
    "    paths = tf.constant(paths)\n",
    "    volumes = tf.constant(volumes, dtype=tf.float32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((paths, volumes))\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=min(1000, len(paths)))\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # Map preprocessing\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: preprocess_image(x, y, is_training),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(train_paths, train_volumes, BATCH_SIZE, is_training=True)\n",
    "val_dataset = create_dataset(val_paths, val_volumes, BATCH_SIZE, is_training=False)\n",
    "\n",
    "\n",
    "# --- 4. MODEL ARCHITECTURE ---\n",
    "def create_model(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
    "    tf.keras.backend.clear_session()\n",
    "    inputs = tf.keras.Input(shape=input_shape, name='input_layer')\n",
    "    x = tf.keras.layers.Lambda(\n",
    "        tf.keras.applications.efficientnet.preprocess_input,\n",
    "        name='effnet_preproc'\n",
    "    )(inputs)\n",
    "\n",
    "    try:\n",
    "        base_model = EfficientNetB0(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=input_shape,  # ‚úÖ ‡πÄ‡∏ô‡πâ‡∏ô 3-channel\n",
    "            pooling=None\n",
    "        )\n",
    "        features = base_model(x)\n",
    "        x = GlobalAveragePooling2D()(features)\n",
    "        logger.info(\"‚úÖ EfficientNetB0 loaded successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading EfficientNetB0: {e}\")\n",
    "        logger.info(\"Using fallback CNN architecture\")\n",
    "        x = tf.keras.layers.Conv2D(32, 3, activation='relu')(x)\n",
    "        x = tf.keras.layers.MaxPooling2D()(x)\n",
    "        x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\n",
    "        x = tf.keras.layers.MaxPooling2D()(x)\n",
    "        x = tf.keras.layers.Conv2D(128, 3, activation='relu')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        base_model = None\n",
    "\n",
    "    # ... (dense head ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)\n",
    "\n",
    "    if base_model is not None:\n",
    "        # Get features from base model\n",
    "        features = base_model.output\n",
    "\n",
    "        # Global average pooling\n",
    "        x = GlobalAveragePooling2D()(features)\n",
    "\n",
    "    # Custom regression head\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    # Output layer - ensure float32 for mixed precision\n",
    "    outputs = Dense(1, activation='linear', dtype='float32', name='volume_output')(x)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='mangosteen_volume_model')\n",
    "\n",
    "    # Freeze base model initially if it exists\n",
    "    if base_model is not None:\n",
    "        base_model.trainable = False\n",
    "        return model, base_model\n",
    "    else:\n",
    "        return model, None\n",
    "\n",
    "\n",
    "# Create model\n",
    "logger.info(\"Building model...\")\n",
    "try:\n",
    "    model, base_model = create_model()\n",
    "    logger.info(\"‚úÖ Model created successfully\")\n",
    "    logger.info(f\"Model has {model.count_params():,} parameters\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- 5. TRAINING CALLBACKS ---\n",
    "\n",
    "callbacks = [\n",
    "    # Save best model\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(CHECKPOINT_DIR, 'best_model_{epoch:02d}_{val_loss:.4f}.h5'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Early stopping\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Learning rate reduction\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # TensorBoard logging\n",
    "    TensorBoard(\n",
    "        log_dir=LOG_DIR,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=False  # Disable to save memory\n",
    "    )\n",
    "]\n",
    "\n",
    "# --- 6. TRAINING PHASES ---\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = max(1, len(train_paths) // BATCH_SIZE)\n",
    "validation_steps = max(1, len(val_paths) // BATCH_SIZE)\n",
    "\n",
    "logger.info(f\"Steps per epoch: {steps_per_epoch}, Validation steps: {validation_steps}\")\n",
    "\n",
    "# Phase 1: Train head only\n",
    "logger.info(\"Phase 1: Training model head...\")\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE_INITIAL),\n",
    "    loss='mse',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "try:\n",
    "    history_head = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=INITIAL_EPOCHS,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    logger.info(\"‚úÖ Phase 1 completed successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Phase 1 training: {e}\")\n",
    "    # Save current model state\n",
    "    model.save(MODEL_SAVE_PATH.replace('.h5', '_phase1_error.h5'))\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "\n",
    "# Phase 2: Fine-tuning (only if base_model exists)\n",
    "if base_model is not None:\n",
    "    logger.info(\"Phase 2: Fine-tuning model...\")\n",
    "\n",
    "    # Unfreeze top layers of base model\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Freeze bottom layers\n",
    "    for layer in base_model.layers[:-30]:  # Keep more layers frozen for stability\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Count trainable parameters\n",
    "    trainable_count = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    logger.info(f\"Trainable parameters in fine-tuning: {trainable_count:,}\")\n",
    "\n",
    "    # Recompile with lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE_FINE_TUNE),\n",
    "        loss='mse',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        history_finetune = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=FINE_TUNE_EPOCHS,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=validation_steps,\n",
    "            initial_epoch=INITIAL_EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        logger.info(\"‚úÖ Phase 2 completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in Phase 2 training: {e}\")\n",
    "else:\n",
    "    logger.info(\"Skipping Phase 2: No base model to fine-tune\")\n",
    "\n",
    "\n",
    "# --- 7. SAVE FINAL MODEL ---\n",
    "\n",
    "# Create prediction wrapper with denormalization\n",
    "class VolumePredictor(tf.keras.Model):\n",
    "    def __init__(self, base_model, mean, std):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.mean = tf.constant(mean, dtype=tf.float32)\n",
    "        self.std = tf.constant(std, dtype=tf.float32)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        normalized_pred = self.base_model(inputs)\n",
    "        return normalized_pred * self.std + self.mean\n",
    "\n",
    "    # Save model\n",
    "    logger.info(f\"Saving final model to {MODEL_SAVE_PATH}\")\n",
    "    try:\n",
    "        model.save(MODEL_SAVE_PATH, save_format='h5')\n",
    "        logger.info(\"‚úÖ Model saved successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving model: {e}\")\n",
    "        # Try saving weights only\n",
    "        model.save_weights(MODEL_SAVE_PATH.replace('.h5', '_weights.h5'))\n",
    "        logger.info(\"‚úÖ Model weights saved successfully\")\n",
    "\n",
    "    # Save normalization parameters\n",
    "    np.savez(\n",
    "        MODEL_SAVE_PATH.replace('.h5', '_params.npz'),\n",
    "        mean=volume_mean,\n",
    "        std=volume_std\n",
    "    )\n",
    "\n",
    "    logger.info(\"‚úÖ Training complete! Model saved successfully.\")\n",
    "    logger.info(f\"üìä TensorBoard logs saved to: {LOG_DIR}\")\n",
    "    logger.info(\"Run 'tensorboard --logdir=./logs/fit' to view training metrics\")\n",
    "\n",
    "\n",
    "# --- 8. EVALUATION ---\n",
    "\n",
    "def evaluate_model(model, dataset, steps, denormalize=True):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    step_count = 0\n",
    "    for images, volumes in dataset:\n",
    "        if step_count >= steps:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            preds = model.predict(images, verbose=0)\n",
    "            predictions.extend(preds.flatten())\n",
    "            actuals.extend(volumes.numpy())\n",
    "            step_count += 1\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in prediction step {step_count}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not predictions:\n",
    "        logger.error(\"No predictions were made\")\n",
    "        return {}\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    if denormalize:\n",
    "        predictions = predictions * volume_std + volume_mean\n",
    "        actuals = actuals * volume_std + volume_mean\n",
    "\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    mse = np.mean((predictions - actuals) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Avoid division by zero in MAPE calculation\n",
    "    non_zero_mask = actuals != 0\n",
    "    if np.any(non_zero_mask):\n",
    "        mape = np.mean(np.abs((actuals[non_zero_mask] - predictions[non_zero_mask]) / actuals[non_zero_mask])) * 100\n",
    "    else:\n",
    "        mape = float('inf')\n",
    "\n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'num_samples': len(predictions)\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate on validation set\n",
    "logger.info(\"\\nEvaluating model performance...\")\n",
    "try:\n",
    "    metrics = evaluate_model(model, val_dataset, validation_steps)\n",
    "    if metrics:\n",
    "        for metric, value in metrics.items():\n",
    "            if metric == 'num_samples':\n",
    "                logger.info(f\"{metric}: {value}\")\n",
    "            else:\n",
    "                logger.info(f\"{metric}: {value:.4f}\")\n",
    "    else:\n",
    "        logger.warning(\"Could not evaluate model performance\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during evaluation: {e}\")\n",
    "\n",
    "logger.info(\"Script completed!\")"
   ],
   "id": "bae80bb69023bef7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
